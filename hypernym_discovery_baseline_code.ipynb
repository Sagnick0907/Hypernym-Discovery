{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: Hypernym Discovery**\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rnngWObW_iH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Team Name: Semantic Insighters  "
      ],
      "metadata": {
        "id": "sAlv3uIl_v8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding Generation"
      ],
      "metadata": {
        "id": "wsJxmRh-7CFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing datasets"
      ],
      "metadata": {
        "id": "tXCk0Sde_E70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENGLISH_HYPONYMS_TRAIN_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/training/data/1A.english.training.data.txt\"\n",
        "ENGLISH_HYPERNYMS_TRAIN_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
        "ENGLISH_HYPONYMS_DEV_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/trial/data/1A.english.trial.data.txt\"\n",
        "ENGLISH_HYPONYMS_TEST_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/test/data/1A.english.test.data.txt\"\n",
        "ENGLISH_HYPERNYMS_DEV_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/trial/gold/2A.medical.trial.gold.txt\"\n",
        "ENGLISH_HYPERNYMS_TEST_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/test/gold/2A.medical.test.gold.txt\"\n",
        "ENGLISH_VOCAB_FILENAME = \"english_merged_vocab.txt\"\n",
        "\n",
        "MEDICAL_HYPONYMS_TRAIN_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/training/data/2A.medical.training.data.txt\"\n",
        "MEDICAL_HYPERNYMS_TRAIN_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/training/gold/2A.medical.training.gold.txt\"\n",
        "MEDICAL_HYPONYMS_TEST_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/test/data/2A.medical.test.data.txt\"\n",
        "MEDICAL_HYPONYMS_DEV_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/trial/data/2A.medical.trial.data.txt\"\n",
        "MEDICAL_HYPERNYMS_DEV_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/trial/gold/2A.medical.trial.gold.txt\"\n",
        "MEDICAL_HYPERNYMS_TEST_FILE = \"/content/drive/MyDrive/SemEval2018-Task9/test/gold/2A.medical.test.gold.txt\"\n",
        "MEDICAL_VOCAB_FILENAME = \"medical_merged_vocab.txt\""
      ],
      "metadata": {
        "id": "VwSiYXFD2TDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(FILENAME):\n",
        "    with open(FILENAME, encoding='utf-8', errors='ignore') as f:\n",
        "          lines = f.readlines()\n",
        "    modified_lines = list()\n",
        "    for line in lines:\n",
        "        words = line.strip().split(\"\\t\")\n",
        "        modified_line = \"\"\n",
        "        for word in words:\n",
        "            underscored_word = \"_\".join(word.split(\" \")).lower()\n",
        "            modified_line += underscored_word + \"\\t\"\n",
        "        modified_lines.append(modified_line)\n",
        "    return modified_lines\n",
        "\n",
        "def merge_hypernyms(hypernyms):\n",
        "    all_hypernyms = list()\n",
        "    for hypers in hypernyms:\n",
        "        all_hypernyms += hypers\n",
        "    return list(set(all_hypernyms))"
      ],
      "metadata": {
        "id": "Uk2l8t5E3roO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Vocabulory"
      ],
      "metadata": {
        "id": "YUiYRIg77Gp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab_file(hyponyms_train_file, hypernyms_train_file, hyponyms_dev_file, hypernyms_dev_file, hyponyms_test_file, hypernyms_test_file, output_filename):\n",
        "    hyponyms_train = [line.strip().split(\"\\t\")[0] for line in read_file(hyponyms_train_file)]\n",
        "    hypernyms_train = merge_hypernyms([line.strip().split(\"\\t\") for line in read_file(hypernyms_train_file)])\n",
        "    hyponyms_dev = [line.strip().split(\"\\t\")[0] for line in read_file(hyponyms_dev_file)]\n",
        "    hypernyms_dev = merge_hypernyms([line.strip().split(\"\\t\") for line in read_file(hypernyms_dev_file)])\n",
        "    hyponyms_test = [line.strip().split(\"\\t\")[0] for line in read_file(hyponyms_test_file)]\n",
        "    hypernyms_test = merge_hypernyms([line.strip().split(\"\\t\") for line in read_file(hypernyms_test_file)])\n",
        "\n",
        "    all_words = list()\n",
        "\n",
        "    all_words += hyponyms_train\n",
        "    all_words += hypernyms_train\n",
        "    all_words += hyponyms_dev\n",
        "    all_words += hypernyms_dev\n",
        "    all_words += hyponyms_test\n",
        "    all_words += hypernyms_test\n",
        "\n",
        "    new_vocab = list(set(all_words))\n",
        "\n",
        "    with open(output_filename, \"w\") as f:\n",
        "        for word in new_vocab:\n",
        "            f.write(word + \"\\n\")\n",
        "\n",
        "    print(\"New vocab file {} created successfully.\".format(output_filename))"
      ],
      "metadata": {
        "id": "oUrwccrz3sZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_vocab_file(ENGLISH_HYPONYMS_TRAIN_FILE, ENGLISH_HYPERNYMS_TRAIN_FILE,\n",
        "                  ENGLISH_HYPONYMS_DEV_FILE, ENGLISH_HYPERNYMS_DEV_FILE,\n",
        "                  ENGLISH_HYPONYMS_TEST_FILE, ENGLISH_HYPERNYMS_TEST_FILE,\n",
        "                  ENGLISH_VOCAB_FILENAME)\n",
        "\n",
        "create_vocab_file(MEDICAL_HYPONYMS_TRAIN_FILE, MEDICAL_HYPERNYMS_TRAIN_FILE,\n",
        "                  MEDICAL_HYPONYMS_DEV_FILE, MEDICAL_HYPERNYMS_DEV_FILE,\n",
        "                  MEDICAL_HYPONYMS_TEST_FILE, MEDICAL_HYPERNYMS_TEST_FILE,\n",
        "                  MEDICAL_VOCAB_FILENAME)"
      ],
      "metadata": {
        "id": "JMnvJeIX3zBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Dataloaders"
      ],
      "metadata": {
        "id": "XHyB_SJy7Kiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "c5_-ReI8-5w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXLbsCG5wjBq"
      },
      "outputs": [],
      "source": [
        "class Hypernymy_Dataset(Dataset):\n",
        "    def __init__(self, HYPONYMS_FILENAME, HYPERNYMS_FILENAME, VOCAB_FILENAME, num_negs=5):\n",
        "        self.hyponyms = [line.strip().split(\"\\t\")[0] for line in self.read_file(HYPONYMS_FILENAME)]\n",
        "        self.hypernyms = [line.strip().split(\"\\t\") for line in self.read_file(HYPERNYMS_FILENAME)]\n",
        "        self.vocab = [line.strip().split(\"\\t\")[0] for line in self.read_file(VOCAB_FILENAME)]\n",
        "        self.all_hypernyms = self.merge_hypernyms()\n",
        "        self.num_negs = num_negs\n",
        "        self.word_id_map = {}\n",
        "        self.id_word_map = {}\n",
        "        for ndx, word in enumerate(self.vocab):\n",
        "            self.word_id_map[word] = ndx\n",
        "            self.id_word_map[ndx] = word\n",
        "        self.data_size = len(self.hyponyms)\n",
        "\n",
        "        self.generate_mappings()\n",
        "\n",
        "    def read_file(self, FILENAME):\n",
        "        with open(FILENAME, encoding='utf-8', errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "        modified_lines = []\n",
        "        for line in lines:\n",
        "            words = line.strip().split(\"\\t\")\n",
        "            modified_line = \"\"\n",
        "            for word in words:\n",
        "                underscored_word = \"_\".join(word.split(\" \")).lower()\n",
        "                modified_line += underscored_word + \"\\t\"\n",
        "            modified_lines.append(modified_line.strip())\n",
        "        return modified_lines\n",
        "\n",
        "    def merge_hypernyms(self):\n",
        "        all_hypernyms = []\n",
        "        for hypers in self.hypernyms:\n",
        "            all_hypernyms += hypers\n",
        "        return list(set(all_hypernyms))\n",
        "\n",
        "    def generate_mappings(self):\n",
        "        for ndx, word in enumerate(self.vocab):\n",
        "            self.word_id_map[word] = ndx\n",
        "            self.id_word_map[ndx] = word\n",
        "\n",
        "    def generate_negative_samples(self, ndx):\n",
        "        positives = self.hypernyms[ndx]\n",
        "        negatives = []\n",
        "        count = 0\n",
        "        while count < self.num_negs*len(positives):\n",
        "            rand_neg = self.all_hypernyms[np.random.randint(0, len(self.all_hypernyms))]\n",
        "            if rand_neg not in positives and rand_neg not in negatives:\n",
        "                negatives.append(rand_neg)\n",
        "                count += 1\n",
        "        return negatives\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(batches):\n",
        "        u = [u for b in batches for u, _, _ in b if len(b) > 0]\n",
        "        v = [v for b in batches for _, v, _ in b if len(b) > 0]\n",
        "        neg = [neg for b in batches for _, _, neg in b if len(b) > 0]\n",
        "\n",
        "        return torch.tensor(u), torch.tensor(v), torch.tensor(neg)\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        hyponym = self.hyponyms[ndx]\n",
        "        hypernyms = self.hypernyms[ndx]\n",
        "        negative_samples = self.generate_negative_samples(ndx)\n",
        "        items = []\n",
        "        start, end = 0, self.num_negs\n",
        "        for hypernym in hypernyms:\n",
        "          if hyponym in self.word_id_map and hypernym in self.word_id_map:\n",
        "              negatives = [self.word_id_map[neg] for neg in negative_samples[start:end] if neg in self.word_id_map]\n",
        "              start = end\n",
        "              end += self.num_negs\n",
        "              items.append((self.word_id_map[hyponym],\n",
        "                            self.word_id_map[hypernym],\n",
        "                            negatives))\n",
        "        return items\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hyponyms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxWaY95t7K3L",
        "outputId": "8d1f89b7-b5c9-4d8b-db0b-55cd8a9b0714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sg_dataset = Hypernymy_Dataset(ENGLISH_HYPONYMS_TRAIN_FILE, ENGLISH_HYPERNYMS_TRAIN_FILE, ENGLISH_VOCAB_FILENAME)\n",
        "print(len(sg_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQXVbxvm4Xl8",
        "outputId": "2ab1efe1-99f6-4086-de54-22fe348382f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sg_dataset_medical = Hypernymy_Dataset(MEDICAL_HYPONYMS_TRAIN_FILE, MEDICAL_HYPERNYMS_TRAIN_FILE, MEDICAL_VOCAB_FILENAME)"
      ],
      "metadata": {
        "id": "J6SSuQ9N4YrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sg_dataset_medical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0XcIzt1liUy",
        "outputId": "5a5376e9-2566-4761-90ea-83a1b413b89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding using Skipgram Model"
      ],
      "metadata": {
        "id": "IbzW56_Y7QFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Libraries"
      ],
      "metadata": {
        "id": "TBrk6V837jpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "T1X4et-37dWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture"
      ],
      "metadata": {
        "id": "qbr0B2bm7hic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dimension):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=True)\n",
        "\n",
        "        initrange = 1.0 / self.emb_dimension\n",
        "        init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "      emb_u = self.u_embeddings(pos_u)\n",
        "      emb_v = self.u_embeddings(pos_v)\n",
        "      emb_neg_v = self.u_embeddings(neg_v)\n",
        "\n",
        "      score = torch.sum(emb_u * emb_v, dim=1)\n",
        "      score = -F.logsigmoid(score)\n",
        "      neg_score = torch.sum(-(emb_neg_v.matmul(emb_u.unsqueeze(2)).squeeze()), dim=1)\n",
        "      neg_score = -F.logsigmoid(neg_score)\n",
        "\n",
        "      return torch.mean(score + neg_score)\n",
        "\n",
        "    def save_embedding(self, id_word_map, OUTFILE_NAME):\n",
        "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        with open(OUTFILE_NAME, 'w') as f:\n",
        "            f.write('%d %d\\n' % (self.vocab_size, self.emb_dimension))\n",
        "            for wid, w in id_word_map.items():\n",
        "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
        "                f.write('%s %s\\n' % (w, e))\n"
      ],
      "metadata": {
        "id": "ET9GIqDu2ga6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Initialization"
      ],
      "metadata": {
        "id": "OdelNbtk7otT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sg_dataloader = DataLoader(sg_dataset, batch_size=32, shuffle=True, collate_fn=sg_dataset.collate)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size_eng = len(sg_dataset.vocab)\n",
        "embedding_len = 300\n",
        "word2vec_sg_model = SkipGramModel(vocab_size_eng, embedding_len).to(device)\n",
        "OUTFILE_name_eng = \"english\"\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "-PEOPXJ85RF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Training"
      ],
      "metadata": {
        "id": "lTpA2J857q4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, dataloader, epochs, device, OUTFILE_name):\n",
        "    optimizer = optim.SparseAdam(word2vec_sg_model.parameters(), lr=0.00001)\n",
        "    loss_logs = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        running_loss = 0.0\n",
        "        for i, sample_batched in enumerate(dataloader):\n",
        "            if len(sample_batched[0]) > 1:\n",
        "                pos_u = sample_batched[0].to(device)\n",
        "                pos_v = sample_batched[1].to(device)\n",
        "                neg_v = sample_batched[2].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = model(pos_u, pos_v, neg_v)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss}\")\n",
        "        loss_logs.append(running_loss)\n",
        "\n",
        "    model.save_embedding(dataset.id_word_map, OUTFILE_name + \"_sg_embed.txt\")\n",
        "\n",
        "    return loss_logs"
      ],
      "metadata": {
        "id": "6qqhcvwzCioQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(word2vec_sg_model, sg_dataset, sg_dataloader, epochs, device, OUTFILE_name_eng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr6nx8u_i0tB",
        "outputId": "686ce065-62e6-4d09-af13-e965b444d28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:01<00:09,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.3764923380693797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:01<00:07,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 1.3764882917768733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:02<00:06,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 1.376485367245891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:04<00:06,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 1.3764788227309286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:05<00:05,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 1.3764719413280901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:06<00:04,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 1.37646724532988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:07<00:03,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 1.376462029616177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:08<00:02,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 1.3764573591073914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [00:09<00:01,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 1.376450362706667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:10<00:00,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.3764440483107785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3764923380693797,\n",
              " 1.3764882917768733,\n",
              " 1.376485367245891,\n",
              " 1.3764788227309286,\n",
              " 1.3764719413280901,\n",
              " 1.37646724532988,\n",
              " 1.376462029616177,\n",
              " 1.3764573591073914,\n",
              " 1.376450362706667,\n",
              " 1.3764440483107785]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sg_dataloader_medical = DataLoader(sg_dataset_medical, batch_size=32, shuffle=True, collate_fn=sg_dataset_medical.collate)\n",
        "vocab_size_med = len(sg_dataset_medical.vocab)\n",
        "word2vec_sg_model_med = SkipGramModel(vocab_size_med, embedding_len).to(device)\n",
        "OUTFILE_name_med = \"medical\"\n",
        "train(word2vec_sg_model_med, sg_dataset_medical, sg_dataloader_medical, epochs, device, OUTFILE_name_med)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvpja3HujzHP",
        "outputId": "09941c17-ba9f-4681-9bad-60ea7507b76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:00<00:03,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.1294110627291358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:01<00:04,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 1.1294139980778541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:02<00:05,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 1.1294143866665842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:03<00:06,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 1.1294117478907326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:05<00:06,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 1.1294136441314913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:07<00:06,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 1.1294119358432149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:10<00:06,  2.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 1.1294123254583848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:13<00:04,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 1.1294119304338195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [00:16<00:02,  2.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 1.1294127939634486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:20<00:00,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.129412335080064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1294110627291358,\n",
              " 1.1294139980778541,\n",
              " 1.1294143866665842,\n",
              " 1.1294117478907326,\n",
              " 1.1294136441314913,\n",
              " 1.1294119358432149,\n",
              " 1.1294123254583848,\n",
              " 1.1294119304338195,\n",
              " 1.1294127939634486,\n",
              " 1.129412335080064]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypernym Prediction"
      ],
      "metadata": {
        "id": "pvMSyyC9694r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function"
      ],
      "metadata": {
        "id": "Eno9Dgcz7vnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hypernym_discovery_baseline(task, model=\"gru\"):\n",
        "    if task == \"1A\":\n",
        "        dataset = \"English\"\n",
        "        english_embeddings_file = open(\"english_sg_embed.txt\", \"r\")\n",
        "        embeddings = english_embeddings_file.read().splitlines()\n",
        "        training_set = \"/content/drive/MyDrive/SemEval2018-Task9/training/data/1A.english.training.data.txt\"\n",
        "        training_hypernym_set = \"/content/drive/MyDrive/SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
        "        testing_set = \"/content/drive/MyDrive/SemEval2018-Task9/test/data/1A.english.test.data.txt\"\n",
        "        testing_hypernym_set = \"/content/drive/MyDrive/SemEval2018-Task9/test/gold/1A.english.test.gold.txt\"\n",
        "        validation_set = \"/content/drive/MyDrive/SemEval2018-Task9/trial/data/1A.english.trial.data.txt\"\n",
        "        validation_hypernym_set = \"/content/drive/MyDrive/SemEval2018-Task9/trial/gold/1A.english.trial.gold.txt\"\n",
        "        if model == \"gru\":\n",
        "            hypernyms_to_be_saved_for_trainset_data = \"train_predicted_hypernym_gru_english.txt\"\n",
        "            hypernyms_to_be_saved_for_testset_data = \"test_predicted_hypernym_gru_english.txt\"\n",
        "        else:\n",
        "            hypernyms_to_be_saved_for_trainset_data = \"train_predicted_hypernym_lstm_english.txt\"\n",
        "            hypernyms_to_be_saved_for_testset_data = \"test_predicted_hypernym_lstm_english.txt\"\n",
        "\n",
        "    elif task == \"2A\":\n",
        "        dataset = \"Medical\"\n",
        "        medical_embeddings_file = open(\"medical_sg_embed.txt\", \"r\")\n",
        "        embeddings = medical_embeddings_file.read().splitlines()\n",
        "        training_set = \"/content/drive/MyDrive/SemEval2018-Task9/training/data/2A.medical.training.data.txt\"\n",
        "        training_hypernym_set = \"/content/drive/MyDrive/SemEval2018-Task9/training/gold/2A.medical.training.gold.txt\"\n",
        "        testing_set = \"/content/drive/MyDrive/SemEval2018-Task9/test/data/2A.medical.test.data.txt\"\n",
        "        testing_hypernym_set = \"/content/drive/MyDrive/SemEval2018-Task9/test/gold/2A.medical.test.gold.txt\"\n",
        "        validation_set = \"/content/drive/MyDrive/SemEval2018-Task9/trial/data/2A.medical.trial.data.txt\"\n",
        "        validation_hypernym_set = \"/content/drive/MyDrive/SemEval2018-Task9/trial/gold/2A.medical.trial.gold.txt\"\n",
        "        if model == \"gru\":\n",
        "            hypernyms_to_be_saved_for_trainset_data = \"train_predicted_hypernym_gru_medical.txt\"\n",
        "            hypernyms_to_be_saved_for_testset_data = \"test_predicted_hypernym_gru_medical.txt\"\n",
        "        else:\n",
        "            hypernyms_to_be_saved_for_trainset_data = \"train_predicted_hypernym_lstm_medical.txt\"\n",
        "            hypernyms_to_be_saved_for_testset_data = \"test_predicted_hypernym_lstm_medical.txt\"\n",
        "\n",
        "\n",
        "    vocab_size = len(embeddings)\n",
        "    embedding_matrix = np.zeros((vocab_size, 300))\n",
        "    counter = 1\n",
        "    word_vocab = []\n",
        "    word_vocab.append(\"UNK\")\n",
        "    embedding_matrix[0] = np.random.random(300)\n",
        "\n",
        "\n",
        "    for word_embed in embeddings[1:]:\n",
        "        word_vocab.append(word_embed.split()[0])\n",
        "        temp_embedding = word_embed.strip().split(' ')[1:]\n",
        "        temp_array = np.zeros(shape=(1, 300))\n",
        "\n",
        "        for i in range(len(temp_embedding)):\n",
        "            temp_array[0, i] = np.float64(temp_embedding[i])\n",
        "        embedding_matrix[counter] = temp_array[0]\n",
        "        counter = counter + 1\n",
        "\n",
        "    queryset_file = open(training_set, \"r\")\n",
        "    queryset = queryset_file.readlines()\n",
        "\n",
        "    for i in range(len(queryset)):\n",
        "        queryset[i] = \"_\".join(queryset[i].split(\"\\t\")[0].split())\n",
        "\n",
        "    queryset_test_file = open(testing_set, \"r\")\n",
        "    queryset_test = queryset_test_file.readlines()\n",
        "\n",
        "    for i in range(len(queryset_test)):\n",
        "        queryset_test[i] = \"_\".join(queryset_test[i].split(\"\\t\")[0].split())\n",
        "\n",
        "    queryset_validation_file = open(validation_set, \"r\")\n",
        "    queryset_validation = queryset_validation_file.readlines()\n",
        "\n",
        "    for i in range(len(queryset_validation)):\n",
        "        queryset_validation[i] = \"_\".join(queryset_validation[i].split(\"\\t\")[0].split())\n",
        "\n",
        "    training_hypernym_file = open(training_hypernym_set, \"r\")\n",
        "    training_hypernyms = training_hypernym_file.read().splitlines()\n",
        "\n",
        "    testing_hypernym_file = open(testing_hypernym_set, \"r\")\n",
        "    testing_hypernyms = testing_hypernym_file.read().splitlines()\n",
        "\n",
        "    validation_hypernym_file = open(validation_hypernym_set, \"r\")\n",
        "    validation_hypernyms = validation_hypernym_file.read().splitlines()\n",
        "\n",
        "    # Creating the train, test and validation datasets\n",
        "    training_query_hypernym_pair, y_train = dataset_preparation(queryset, word_vocab, training_hypernyms,\n",
        "                                                                embedding_matrix)\n",
        "    testing_query_hypernym_pair, y_test = dataset_preparation(queryset_test, word_vocab, testing_hypernyms,\n",
        "                                                              embedding_matrix)\n",
        "    validation_query_hypernym_pair, y_validation = dataset_preparation(queryset_validation, word_vocab,\n",
        "                                                                       validation_hypernyms, embedding_matrix)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Training the model\n",
        "    trained_model, evaluation_test_set_score = model_training(training_query_hypernym_pair, y_train,\n",
        "                                                              testing_query_hypernym_pair, y_test,\n",
        "                                                              validation_query_hypernym_pair, y_validation,\n",
        "                                                              dataset, model, device)\n",
        "\n",
        "    # Predicting hypernyms\n",
        "    final_total_hypernyms_predicted_trainset = predict_hypernyms(queryset, word_vocab, embedding_matrix, trained_model,\n",
        "                                                                model, device)\n",
        "    final_total_hypernyms_predicted_testset = predict_hypernyms(queryset_test, word_vocab, embedding_matrix,\n",
        "                                                                trained_model,model, device)\n",
        "\n",
        "    # Writing the hypernyms in a text file\n",
        "    write_hypernyms(hypernyms_to_be_saved_for_trainset_data, final_total_hypernyms_predicted_trainset)\n",
        "    write_hypernyms(hypernyms_to_be_saved_for_testset_data, final_total_hypernyms_predicted_testset)"
      ],
      "metadata": {
        "id": "24RHu_esu6Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing the results"
      ],
      "metadata": {
        "id": "_swxpN0a7y05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_hypernyms(hypernyms_to_be_saved, total_hypernyms_predicted):\n",
        "    print(\"within write_hypernyms method\")\n",
        "    hyp_counter = 0\n",
        "    for hyp in total_hypernyms_predicted:\n",
        "        if len(hyp) > 15:\n",
        "            counter_limit = 15  # selecting atmost top 15 hypernyms\n",
        "        else:\n",
        "            counter_limit = len(hyp)\n",
        "\n",
        "        while counter_limit > 0:\n",
        "            with open(hypernyms_to_be_saved, 'a') as f:\n",
        "                f.writelines(\"%s\\t\" % hyp[hyp_counter][0])\n",
        "                print(hyp[hyp_counter][0])\n",
        "            f.close()\n",
        "            hyp_counter = hyp_counter + 1\n",
        "            counter_limit = counter_limit - 1\n",
        "        with open(hypernyms_to_be_saved, 'a') as f:\n",
        "            print(\"------------------\")\n",
        "            hyp_counter = 0\n",
        "            f.writelines(\"\\n\")\n",
        "        f.close()"
      ],
      "metadata": {
        "id": "Xz0QdNcYu90K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture, Training and Prediction"
      ],
      "metadata": {
        "id": "qScBjp7K73Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru1 = nn.GRU(input_size=300, hidden_size=300, num_layers=1, batch_first=True, dropout=0.3)\n",
        "        self.gru2 = nn.GRU(input_size=300, hidden_size=300, num_layers=1, batch_first=True, dropout=0.2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(300, 300)\n",
        "        self.fc2 = nn.Linear(300, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru1(x)\n",
        "        out, _ = self.gru2(out[:, -1, :].unsqueeze(1))\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size=300, hidden_size=300, num_layers=1, batch_first=True, dropout=0.3)\n",
        "        self.lstm2 = nn.LSTM(input_size=300, hidden_size=300, num_layers=1, batch_first=True, dropout=0.2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(300, 300)\n",
        "        self.fc2 = nn.Linear(300, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm1(x)\n",
        "        out, _ = self.lstm2(out[:, -1, :].unsqueeze(1))\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "def model_training(training_query_hypernym_pair, y_train, testing_query_hypernym_pair, y_test,\n",
        "                   validation_query_hypernym_pair, y_validation, dataset, model_type, device, batch_size=32):\n",
        "    loss_fn = nn.BCELoss()\n",
        "    adam_optimiser = optim.Adam\n",
        "\n",
        "    if model_type == \"gru\":\n",
        "        model = GRUModel().to(device)\n",
        "    else:\n",
        "        model = LSTMModel().to(device)\n",
        "\n",
        "    criterion = loss_fn\n",
        "    optimizer = adam_optimiser(model.parameters(), lr=0.003)\n",
        "\n",
        "    X_train_tensor = torch.tensor(training_query_hypernym_pair, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "    X_test_tensor = torch.tensor(testing_query_hypernym_pair, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "    X_val_tensor = torch.tensor(validation_query_hypernym_pair, dtype=torch.float32).to(device)\n",
        "    y_val_tensor = torch.tensor(y_validation, dtype=torch.float32).to(device)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            targets = targets.unsqueeze(1)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            targets = targets.unsqueeze(1)\n",
        "            eval_loss += criterion(outputs, targets).item() * inputs.size(0)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    evaluation_test_set_score = eval_loss / total_samples\n",
        "\n",
        "    torch.save(model.state_dict(), f\"{model_type}_{dataset}_trained_model.pth\")\n",
        "\n",
        "    return model, evaluation_test_set_score\n",
        "\n",
        "def predict_hypernyms(queryset, given_vocab, embedding_matrix, model, model_type=\"gru\", device='cuda'):\n",
        "    vocab_size = len(given_vocab)\n",
        "    prediction_array = np.zeros((vocab_size - 1, 2, 300))\n",
        "    total_hypernyms_predicted = []\n",
        "    target_hypernyms = []\n",
        "\n",
        "    for query in queryset:\n",
        "        if query in given_vocab:\n",
        "            index = given_vocab.index(query)\n",
        "            query_embed = embedding_matrix[index]\n",
        "        else:\n",
        "            query_embed = embedding_matrix[0]\n",
        "\n",
        "        for j in range(vocab_size - 1):\n",
        "            prediction_array[j][0] = query_embed\n",
        "            prediction_array[j][1] = embedding_matrix[j + 1]\n",
        "\n",
        "        prediction_array_tensor = torch.tensor(prediction_array, dtype=torch.float32).to(device)\n",
        "        predicted_hypernyms = model(prediction_array_tensor).detach().cpu().numpy()\n",
        "\n",
        "        for z in range(len(predicted_hypernyms)):\n",
        "            if predicted_hypernyms[z][0] < 0.5:\n",
        "                continue\n",
        "            else:\n",
        "                target_hypernyms.append([given_vocab[z], predicted_hypernyms[z][0]])\n",
        "        total_hypernyms_predicted.append(target_hypernyms)\n",
        "        target_hypernyms = []\n",
        "\n",
        "    for i in range(len(total_hypernyms_predicted)):\n",
        "        total_hypernyms_predicted[i] = sorted(total_hypernyms_predicted[i], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return total_hypernyms_predicted"
      ],
      "metadata": {
        "id": "FtG97xrU6DHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Preparation"
      ],
      "metadata": {
        "id": "I5VTQKCJ8BPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "def dataset_preparation(queryset, word_vocab, hypernyms_tab_sep, embedding_matrix):\n",
        "    # generating a list of query-hypernym pairs for given corpus using embedding\n",
        "    query_hypernym_training_embedding = []\n",
        "    y_label = []\n",
        "\n",
        "    for i in range(len(queryset)):\n",
        "        if queryset[i] in word_vocab:\n",
        "            index = word_vocab.index(queryset[i])\n",
        "            query_embedd = torch.tensor(embedding_matrix[index], dtype=torch.float32)\n",
        "\n",
        "            for hypernyms_found in hypernyms_tab_sep[i].split(\"\\t\"):\n",
        "                hypernyms_found = \"_\".join(hypernyms_found.split())\n",
        "                if hypernyms_found in word_vocab:\n",
        "                    hypernym_index = word_vocab.index(hypernyms_found)\n",
        "                    hypernym_embedd = torch.tensor(embedding_matrix[hypernym_index], dtype=torch.float32)\n",
        "                    query_hypernym_pair = torch.stack((query_embedd, hypernym_embedd), dim=0)\n",
        "                    query_hypernym_training_embedding.append(query_hypernym_pair)\n",
        "                    y_label.append(1)  # positive label for each positive pair\n",
        "\n",
        "                    # generating 5 negative samples per positive sample\n",
        "                    negative_samples_counter = 0\n",
        "\n",
        "                    while negative_samples_counter == 5:\n",
        "                        random_index = random.randint(0, len(word_vocab) - 1)\n",
        "                        if random_index != index:\n",
        "                            negative_hypernym_embedd = torch.tensor(embedding_matrix[random_index], dtype=torch.float32)\n",
        "                            negative_pair = torch.stack((query_embedd, negative_hypernym_embedd), dim=0)\n",
        "                            query_hypernym_training_embedding.append(negative_pair)\n",
        "                            y_label.append(0)  # negative label for negative pairs\n",
        "                            negative_samples_counter += 1\n",
        "                else:\n",
        "                    print(hypernyms_found)\n",
        "                    query_embedd = torch.tensor(embedding_matrix[index], dtype=torch.float32)\n",
        "                    hypernym_index = 0\n",
        "                    hypernym_embedd = torch.tensor(embedding_matrix[hypernym_index], dtype=torch.float32)\n",
        "                    query_hypernym_pair = torch.stack((query_embedd, hypernym_embedd), dim=0)\n",
        "                    query_hypernym_training_embedding.append(query_hypernym_pair)\n",
        "                    y_label.append(1)  # positive label for each positive pair\n",
        "\n",
        "                    # generating 5 negative samples per positive sample\n",
        "                    negative_samples_counter = 0\n",
        "                    while negative_samples_counter == 5:\n",
        "                        random_index = random.randint(0, len(word_vocab) - 1)\n",
        "                        if random_index != index:\n",
        "                            negative_hypernym_embedd = torch.tensor(embedding_matrix[random_index], dtype=torch.float32)\n",
        "                            negative_pair = torch.stack((query_embedd, negative_hypernym_embedd), dim=0)\n",
        "                            query_hypernym_training_embedding.append(negative_pair)\n",
        "                            y_label.append(0)  # negative label for negative pairs\n",
        "                            negative_samples_counter += 1\n",
        "\n",
        "    # Shuffling the dataset\n",
        "    combined_dataset = list(zip(query_hypernym_training_embedding, y_label))\n",
        "    random.shuffle(combined_dataset)\n",
        "    query_hypernym_training_embedding, y_label = zip(*combined_dataset)\n",
        "\n",
        "    query_hypernym_training_embedding = torch.stack(query_hypernym_training_embedding, dim=0)\n",
        "    y_label = torch.tensor(y_label, dtype=torch.float32)\n",
        "\n",
        "    return query_hypernym_training_embedding, y_label"
      ],
      "metadata": {
        "id": "dVp1R1tvzcE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call main funtion for dataset 1A\n",
        "hypernym_discovery_baseline(\"1A\", \"gru\")\n",
        "hypernym_discovery_baseline(\"1A\", \"lstm\")"
      ],
      "metadata": {
        "id": "DLryGrA1aViF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}