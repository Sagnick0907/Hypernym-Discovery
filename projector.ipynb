{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport random\nfrom math import sqrt\nfrom itertools import cycle\nfrom copy import deepcopy\nfrom time import time\n\nclass ProjectionModel(nn.Module):\n    def __init__(self, embedding_layer):\n        super(ProjectionModel, self).__init__()\n        self.embedding_layer = embedding_layer\n        self.dropout = nn.Dropout(0.5)\n        variance = 2 / (300 + 300)\n        self.projection_matrices = nn.Parameter(\n            torch.eye(300, device=embedding_layer.weight.device).unsqueeze(0).repeat(24, 1, 1)\n        )\n        self.projection_matrices.data.normal_(0, variance)\n        self.projection_matrices.data += torch.eye(300, device=embedding_layer.weight.device).unsqueeze(0).repeat(24, 1, 1)\n\n    def get_projections(self, embeddings):\n        if self.training:\n            embeddings = self.dropout(embeddings)\n        projections = torch.matmul(self.projection_matrices, embeddings.transpose(0, 1))\n        if self.training:\n            projections = self.dropout(projections)\n        projections = projections.transpose(0, 1).transpose(0, 2)\n        return projections\n\n    def forward(self, query_embeddings, candidate_indices):\n        projected_queries = self.get_projections(query_embeddings)\n        candidate_embeddings = self.embedding_layer(candidate_indices)\n        if self.training:\n            candidate_embeddings = self.dropout(candidate_embeddings)\n        candidate_embeddings = candidate_embeddings.transpose(1, 2)\n        features = torch.bmm(projected_queries, candidate_embeddings)\n        return features\n\n\nclass ClassifierModel(nn.Module:\n    def __init__(self, projection_model):\n        super(ClassifierModel, self).__init__()\n        self.projection_model = projection_model\n        self.output_layer = nn.Linear(24, 1).to(projection_model.projection_matrices.device)\n        self.loss_function = nn.BCEWithLogitsLoss(reduction='sum')\n        self.sigmoid = nn.Sigmoid()\n\n    def forward_to_logits(self, query_embeddings, candidate_indices):\n        features = self.projection_model(query_embeddings, candidate_indices)\n        logits = self.output_layer(features.transpose(1, 2)).squeeze(2)\n        return logits\n\n    def calculate_loss(self, query_embeddings, candidate_indices, targets):\n        logits = self.forward_to_logits(query_embeddings, candidate_indices)\n        loss = self.loss_function(logits, targets)\n        return loss\n\n    def forward(self, query_embeddings, candidate_indices):\n        logits = self.forward_to_logits(query_embeddings, candidate_indices)\n        return self.sigmoid(logits.clamp(-10, 10))\n\nclass Evaluator:\n    def __init__(self, model, query_embeddings, candidate_ids, embedding_layer):\n        self.model = model\n        self.query_embeddings = query_embeddings\n        self.candidate_ids = candidate_ids\n        self.embedding_layer = embedding_layer\n\n    def get_map(self, gold_ids):\n        # This function should compute the Mean Average Precision given gold standard ids\n        # Placeholder for actual MAP calculation\n        return np.mean([random.random() for _ in gold_ids])\n\ndef make_sampler(data):\n    \"\"\"Create a generator that shuffles data each cycle.\"\"\"\n    num_items = len(data)\n    shuffled_items = deepcopy(data)\n    while True:\n        random.shuffle(shuffled_items)\n        for item in shuffled_items:\n            yield item\n\ndef gold_ids(query_embeddings, pairs):\n    \"\"\"Extract sets of gold hypernym IDs for each query based on training pairs.\"\"\"\n    num_queries = query_embeddings.weight.shape[0]\n    query_gold_ids = [set() for _ in range(num_queries)]\n    for query_id, hyper_id in pairs:\n        query_gold_ids[query_id].add(hyper_id)\n    return query_gold_ids\ndef gen_hyponyms(path):\n  with open(path,\"r\",encoding=\"utf-8\") as fp:\n    lines = fp.readlines()\n\n  ho = []\n  for l in lines:\n    parts = l.strip().split(\"\\t\")\n    if len(parts) != 2:\n        continue\n    ws,ty = l.strip().split(\"\\t\")\n    # print(ws)\n    w1 = \"_\".join(ws.split(\" \")).lower()\n    ho.append(w1)\n\n  return ho\n\n\ndef gen_hypernyms(path):\n  with open(path,\"r\",encoding=\"utf-8\") as fp:\n    lines = fp.readlines()\n\n  hy = []\n  for l in lines:\n#     parts = l.strip().split(\"\\t\")\n#     if len(parts) != 2:\n#         continue\n    ws = l.strip().split(\"\\t\")\n    m= []\n    for w in ws:\n      w1 = \"_\".join(w.split(\" \")).lower()\n      m.append(w1)\n    hy.append(m)\n\n  return hy\n\ndef embeddings_dict(embeddings):\n  word2vec_d = dict()\n  word_vocab = []\n  for word_embed in embeddings[1:]:\n    word_vocab.append(word_embed.split()[0])\n    temp_embedding = word_embed.strip().split(' ')[1:]\n    temp_array = np.zeros(shape=(1, 300),dtype=np.float32)\n\n    for i in range(len(temp_embedding)):\n      print(temp_embedding[i])\n      temp_array[0, i] = float(temp_embedding[i])\n\n    word2vec_d[word_embed.split()[0]] = temp_array[0]\n\n  return word2vec_d,word_vocab\ndef train_model(model, query_train_emb, query_trial_emb, train_pairs, trial_pairs, query_trial_ids):\n    max_epochs = 1000\n    patience = 200\n    batch_size = 32\n    clip_threshold = 1e-4\n\n    candidate_ids = list(range(model.projector.embedding_layer.weight.shape[0]))\n    candidate_sampler = make_sampler(candidate_ids)\n\n    hypernym_freq = {hid: 0 for _, hid in train_pairs}\n    for _, hid in train_pairs:\n        hypernym_freq[hid] += 1\n    min_freq = min(hypernym_freq.values())\n\n    pos_sample_prob = {hid: sqrt(min_freq / freq) for hid, freq in hypernym_freq.items()}\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n    best_model, best_score = None, float('-inf')\n    start_time = time()\n\n    for epoch in range(1, max_epochs + 1):\n        model.train()\n        random.shuffle(train_pairs)\n        total_pos_loss, total_neg_loss, total_loss, updates = 0, 0, 0, 0\n\n        batch_queries, batch_pos, batch_neg = [], [], []\n        for query_id, hyper_id in train_pairs:\n            if random.random() < pos_sample_prob[hyper_id]:\n                batch_queries.append(query_id)\n                batch_pos.append(hyper_id)\n                neg_samples = [next(candidate_sampler) for _ in range(10) if next(candidate_sampler) not in train_gold_ids[query_id]]\n                batch_neg.append(neg_samples)\n\n                if len(batch_queries) == batch_size:\n                    losses = process_batch(model, optimizer, batch_queries, batch_pos, batch_neg, clip_threshold, device)\n                    total_pos_loss += losses[0]\n                    total_neg_loss += losses[1]\n                    total_loss += losses[2]\n                    updates += 1\n                    batch_queries, batch_pos, batch_neg = [], [], []  # Reset for next batch\n\n        avg_pos_loss = total_pos_loss / updates\n        avg_neg_loss = total_neg_loss / updates\n        avg_total_loss = total_loss / updates\n\n        trial_loss, MAP = evaluate_model(model, query_trial_emb, trial_pairs, query_trial_ids, trial_gold_ids)\n        print_epoch_results(epoch, avg_pos_loss, avg_neg_loss, avg_total_loss, trial_loss, MAP, start_time)\n\n        if MAP > best_score:\n            best_score = MAP\n            best_model = deepcopy(model)\n            no_gain = 0\n        else:\n            no_gain += 1\n\n        if no_gain >= patience:\n            print(\"EARLY STOP!\")\n            break\n\n    return best_model\n\n# Assuming that some functions like process_batch, evaluate_model, and print_epoch_results are defined elsewhere.\n# This is a basic setup and does not include the full implementation of all functions used in the training process.\nif __name__ == '__main__':\n\n\n    # p = argparse.ArgumentParser()\n    # p.add_argument('domain',type=str)\n    # domain = p.parse_args().domain\n    domain = \"italian\"\n    if(domain == \"english\"):\n        # path = \"/content/english_merged_vocab.txt\"\n        path = \"/kaggle/input/vocabulary/1A.english.vocabulary.txt\"\n        path1 = \"/kaggle/input/training/1A.english.training.data.txt\"\n        path2 = \"/kaggle/input/trailll/1A.english.trial.data.txt\"\n        path3 = \"/kaggle/input/testdata/1A.english.test.data.txt\"\n        pathh1 = \"/kaggle/input/training/1A.english.training.gold.txt\"\n        pathh2 = \"/kaggle/input/trailll/1A.english.trial.gold.txt\"\n        e_path = \"/kaggle/input/engdata/english_sg_embed.txt\"\n        # log_path = \"logs/english_logs.txt\"\n        p_file = \"english_results.txt\"\n        model = \"english_final.pt\"\n    elif(domain == \"italian\"):\n        path = \"/kaggle/input/vocabulary/1B.italian.vocabulary.txt\"\n        path1 = \"/kaggle/input/training/1B.italian.training.data.txt\"\n        path2 = \"/kaggle/input/trailll/1B.italian.trial.data.txt\"\n        path3 = \"/kaggle/input/testdata/1B.italian.test.data.txt\"\n        pathh1 = \"/kaggle/input/training/1B.italian.training.gold.txt\"\n        pathh2 = \"/kaggle/input/trailll/1B.italian.trial.gold.txt\"\n        e_path = \"/kaggle/input/vocabulary/1B.italian.vocabulary.txt\"\n#         log_path = \"logs/italian_logs.txt\"\n        p_file = \"italian_results.txt\"\n        model = \"italian_final.pt\"\n    elif(domain == \"spanish\"):\n        path = \"/content/drive/MyDrive/SemEval2018-Task9/vocabulary/1C.spanish.vocabulary.txt\"\n        path1 = \"/content/drive/MyDrive/SemEval2018-Task9/training/data/1C.spanish.training.data.txt\"\n        path2 = \"/content/drive/MyDrive/SemEval2018-Task9/trial/data/1C.spanish.trial.data.txt\"\n        path3 = \"/content/drive/MyDrive/SemEval2018-Task9/test/data/1C.spanish.test.data.txt\"\n        pathh1 = \"/content/drive/MyDrive/SemEval2018-Task9/training/gold/1C.spanish.training.gold.txt\"\n        pathh2 = \"/content/drive/MyDrive/SemEval2018-Task9/trial/gold/1C.spanish.trial.gold.txt\"\n        e_path = \"embeddings/spanish_embeddings.txt\"\n        log_path = \"logs/spanish_logs.txt\"\n        p_file = \"results/spanish_results.txt\"\n        model = \"models/spanish_final.pt\"\n    elif(domain == \"medical\"):\n        path = \"/kaggle/input/semevaldata/vocabulary/2A.medical.vocabulary.txt\"\n        path1 = \"/kaggle/input/semevaldata/training/data/2A.medical.training.data.txt\"\n        path2 = \"/kaggle/input/semevaldata/trial/data/2A.medical.trial.data.txt\"\n        path3 = \"/kaggle/input/semevaldata/test/data/2A.medical.test.data.txt\"\n        pathh1 = \"/kaggle/input/semevaldata/training/gold/2A.medical.training.gold.txt\"\n        pathh2 = \"/kaggle/input/semevaldata/trial/gold/2A.medical.trial.gold.txt\"\n        e_path = \"/kaggle/input/medmusic/medical_sg_embed.txt\"\n        log_path = \"logs/medical_logs.txt\"\n        p_file = \"medical_results.txt\"\n        model = \"medical_final.pt\"\n    elif(domain == \"music\"):\n        path = \"/kaggle/input/semevaldata/vocabulary/2B.music.vocabulary.txt\"\n        path1 = \"/kaggle/input/semevaldata/training/data/2B.music.training.data.txt\"\n        path2 = \"/kaggle/input/semevaldata/trial/data/2B.music.trial.data.txt\"\n        path3 = \"/kaggle/input/semevaldata/test/data/2B.music.test.data.txt\"\n        pathh1 = \"/kaggle/input/semevaldata/training/gold/2B.music.training.gold.txt\"\n        pathh2 = \"/kaggle/input/semevaldata/trial/gold/2B.music.trial.gold.txt\"\n        e_path = \"/kaggle/input/medmusic/music_sg_embed.txt\"\n        log_path = \"logs/music_logs.txt\"\n        p_file = \"music_results.txt\"\n        model = \"music_final.pt\"\n    else:\n      print(\"wrong argument\")\n      exit()\n    candidates = gen_vocab(path)\n    # print(candidates)\n\n\n\n    q_train= gen_hyponyms(path1)\n    q_trial = gen_hyponyms(path2)\n    q_test = gen_hyponyms(path3)\n\n\n    h_train = gen_hypernyms(pathh1)\n    h_trial= gen_hypernyms(pathh2)\n\n\n\n    embeddings_file = open(e_path,\"r\",encoding=\"utf-8\")\n    embeddings = embeddings_file.read().splitlines()\n\n    word2vec_d,word_vocab = embeddings_dict(embeddings)\n    word_vocabs = set(word_vocab)\n\n    candidates_embeds = em_matrix(word2vec_d,candidates)\n    qtrain_embeds = em_matrix(word2vec_d,q_train)\n    qtrail_embeds = em_matrix(word2vec_d,q_trial)\n    qtest_embeds = em_matrix(word2vec_d,q_test)\n\n    candidate_to_id = inv_dict(candidates)\n    qtrain_to_id = inv_dict(q_train)\n    qtrial_to_id = inv_dict(q_trial)\n\n    # print(candidate_to_id)\n    # print(qtrial_to_id)\n\n    train_pairs = hypo_hyper(q_train,h_train,qtrain_to_id,candidate_to_id)\n    trial_pairs = hypo_hyper(q_trial,h_trial,qtrial_to_id,candidate_to_id)\n\n    qtrain_candids = gen_cand_ids(q_train,candidate_to_id)\n    qtrial_candids = gen_cand_ids(q_trial,candidate_to_id)\n    qtest_candids = gen_cand_ids(q_test,candidate_to_id)\n\n    candidate_temb = embedder(candidates_embeds)\n    qtrain_temb = embedder(qtrain_embeds)\n    qtrial_temb = embedder(qtrail_embeds)\n\n    projector1 = Projector1(candidate_temb)\n    print(projector1)\n\n    classifier1 = Classifier1(projector1)\n    print(classifier1)\n\n    trainables = list(filter(lambda x:x.requires_grad,classifier1.parameters()))\n    trainables.append(qtrain_temb.weight)\n\n    optimizer = torch.optim.Adam(trainables,lr=2e-4,betas=(0.9,0.9),eps=1e-8,weight_decay=0)","metadata":{},"execution_count":null,"outputs":[]}]}